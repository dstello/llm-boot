{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable, evaluate\n",
    "from openai import OpenAI\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(query: str, \n",
    "               weaviate_client, \n",
    "               openai_client, \n",
    "               collection_name: str = \"TeslaCybertruck\",\n",
    "               num_chunks: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Get an answer to a question using RAG (Retrieval Augmented Generation).\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        weaviate_client: Initialized Weaviate client\n",
    "        openai_client: Initialized OpenAI client\n",
    "        collection_name (str): Name of the Weaviate collection to query\n",
    "        num_chunks (int): Number of relevant chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated answer\n",
    "    \"\"\"\n",
    "    # 1. Generate embedding for the query\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        input=query\n",
    "    )\n",
    "    query_embedding = response.data[0].embedding\n",
    "\n",
    "    # 2. Retrieve relevant chunks from Weaviate\n",
    "    collection = weaviate_client.collections.get(collection_name)\n",
    "    similar_texts = collection.query.near_vector(\n",
    "        near_vector=query_embedding,\n",
    "        limit=num_chunks,\n",
    "        return_properties=[\"text\"],\n",
    "        return_metadata=MetadataQuery(distance=True)\n",
    "    )\n",
    "\n",
    "    # 3. Combine retrieved contexts\n",
    "    context_str = \"\\n\\n---\\n\\n\".join(\n",
    "        [doc.properties[\"text\"] for doc in similar_texts.objects]\n",
    "    )\n",
    "    \n",
    "    # 4. Create prompt for GPT\n",
    "    prompt = f\"\"\"Answer the question using ONLY the information provided in the context below. \n",
    "    Do not add any general knowledge or information not contained in the context.\n",
    "\n",
    "    Context:\n",
    "    {context_str}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    # 5. Generate answer using GPT-4\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def rag_agent(inputs: dict, weaviate_client, openai_client) -> dict:\n",
    "    \"\"\"\n",
    "    RAG agent that processes questions using the get_answer function.\n",
    "    \n",
    "    Args:\n",
    "        inputs (dict): Input dictionary containing messages\n",
    "        weaviate_client: Initialized Weaviate client\n",
    "        openai_client: Initialized OpenAI client\n",
    "    \n",
    "    Returns:\n",
    "        dict: Response in the format expected by LangSmith\n",
    "    \"\"\"\n",
    "    # Extract the question from the input messages\n",
    "    # if there is a message history, use the last message\n",
    "\n",
    "    question = inputs[\"messages\"][-1][\"content\"]\n",
    "    \n",
    "    # Get answer using the RAG pipeline\n",
    "    answer = get_answer(\n",
    "        query=question,\n",
    "        weaviate_client=weaviate_client,\n",
    "        openai_client=openai_client\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"message\": {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": answer\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_system(dataset_name: str = \"rag_evaluation_dataset\", \n",
    "                       experiment_prefix: str = \"RAG Test Dataset Evaluation\"):\n",
    "    \"\"\"\n",
    "    Evaluate the RAG system using questions from a LangSmith dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Name of the LangSmith dataset\n",
    "        experiment_prefix (str): Prefix for the experiment name\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Initialize clients\n",
    "    wcd_url = os.environ[\"WCD_URL\"]\n",
    "    wcd_api_key = os.environ[\"WCD_API_KEY\"]\n",
    "    \n",
    "    weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url=wcd_url,\n",
    "        auth_credentials=Auth.api_key(wcd_api_key),\n",
    "        skip_init_checks=True\n",
    "    )\n",
    "    \n",
    "    base_openai_client = OpenAI()\n",
    "    openai_client = wrap_openai(base_openai_client)\n",
    "    \n",
    "    # Define the evaluator\n",
    "    def answer_relevance_evaluator(run, example) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluates the relevance and accuracy of the RAG system's answer\n",
    "        \"\"\"\n",
    "\n",
    "        question = run.inputs[\"inputs\"][\"messages\"][-1][\"content\"]\n",
    "        generated_answer = run.outputs[\"message\"][\"content\"]\n",
    "        reference_answer = example.outputs[\"message\"][\"content\"]\n",
    "        \n",
    "        print(question)\n",
    "        print(generated_answer)\n",
    "        print(reference_answer)\n",
    "        \n",
    "        evaluation_prompt = f\"\"\"\n",
    "        Question: {question}\n",
    "        \n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Reference Answer: {reference_answer}\n",
    "        \n",
    "        Score the generated answer from 0-5:\n",
    "        5 = Perfect match with reference, complete and accurate\n",
    "        4 = Very good, minor differences from reference\n",
    "        3 = Acceptable, but missing some details or slightly inaccurate\n",
    "        2 = Partially correct but significant omissions or inaccuracies\n",
    "        1 = Mostly incorrect or irrelevant\n",
    "        0 = Completely wrong or unrelated\n",
    "        \n",
    "        Return only the number (0-5).\n",
    "        \"\"\"\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an evaluation assistant. Respond only with a number 0-5.\"},\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            score = int(response.choices[0].message.content.strip())\n",
    "            return {\n",
    "                \"key\": \"answer_relevance\",\n",
    "                \"score\": score / 5,  # Normalize to 0-1\n",
    "                \"explanation\": f\"Answer relevance score: {score}/5\"\n",
    "            }\n",
    "        except ValueError:\n",
    "            return {\n",
    "                \"key\": \"answer_relevance\",\n",
    "                \"score\": 0,\n",
    "                \"explanation\": \"Failed to parse score\"\n",
    "            }\n",
    "    \n",
    "    # Create a wrapped version of rag_agent that includes the clients\n",
    "    def agent(inputs: dict) -> dict:\n",
    "        return rag_agent(inputs, weaviate_client, openai_client)\n",
    "    \n",
    "    # Run the evaluation\n",
    "    results = evaluate(\n",
    "        agent,\n",
    "        data=dataset_name,\n",
    "        evaluators=[answer_relevance_evaluator],\n",
    "        experiment_prefix=experiment_prefix\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    weaviate_client.close()\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    results = evaluate_rag_system()\n",
    "    print(\"Evaluation Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
