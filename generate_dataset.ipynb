{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "load_dotenv()\n",
    "base_client = OpenAI()\n",
    "openai_client = wrap_openai(base_client)\n",
    "langsmith_client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document into large, overlapping chunks. We're not using\n",
    "# the chunks for retrieval, so they don't need to be small.\n",
    "\n",
    "# In fact, since we're using the chunks to generate evaluation\n",
    "# questions, it's better if they have more context.\n",
    "\n",
    "# Note: we don't use the markdown parser because it splits the\n",
    "# chunks by section, and we want to split by chunk size.\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file import FlatReader\n",
    "\n",
    "docs = SimpleDirectoryReader(\n",
    "    input_files=[\"./data_2/Tesla Cybertruck Owners Manual.md\"],\n",
    "    filename_as_id=True,\n",
    "    file_extractor={\n",
    "        \".md\": FlatReader()\n",
    "    }\n",
    ").load_data()\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=400\n",
    ")\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a few of the chunks created by the parser\n",
    "\n",
    "print(f\"\\nTotal number of chunks: {len(nodes)}\\n\\n\")\n",
    "\n",
    "print(\"\\nFirst 3 chunks:\")\n",
    "for i in range(min(3, len(nodes))):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(nodes[i].text)\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leverage multiple prompts to generate a diverse set of questions\n",
    "\n",
    "FACTUAL_PROMPT = \"\"\"Generate 2-3 questions that real Cybertruck owners would actually type into a search bar or ask in an owners' forum. These should feel completely natural and conversational.\n",
    "\n",
    "Write questions as if they were being typed into a search bar or asked in a forum. For example:\n",
    "\n",
    "Instead of:\n",
    "- \"How do I activate the climate control system?\"\n",
    "- \"What should I do if the touchscreen becomes unresponsive?\"\n",
    "- \"How does one optimize range in cold weather?\"\n",
    "\n",
    "Write:\n",
    "- \"how to turn on AC in cybertruck\"\n",
    "- \"screen frozen - what now?\"\n",
    "- \"battery draining fast in cold weather\"\n",
    "\n",
    "Make questions feel real by:\n",
    "1. Using natural search patterns\n",
    "   - \"how to...\"\n",
    "   - \"why is my...\"\n",
    "   - \"help with...\"\n",
    "2. Including context and emotion\n",
    "   - \"stuck at supercharger\"\n",
    "   - \"help! frunk won't open\"\n",
    "   - \"confused about ride height settings\"\n",
    "3. Writing like real people\n",
    "   - Use contractions (I'm, won't, can't)\n",
    "   - OK to use incomplete sentences\n",
    "   - Include emotional context (\"Help!\", \"Confused about...\", \"Worried about...\")\n",
    "4. Adding situational details\n",
    "   - \"in rain\"\n",
    "   - \"with kids\"\n",
    "   - \"while camping\"\n",
    "\n",
    "For each question, evaluate its real-world relevance:\n",
    "- \"common\": Everyday, urgent needs:\n",
    "  * \"trunk won't close\"\n",
    "  * \"phone key not working\"\n",
    "  * \"what's this warning light mean\"\n",
    "\n",
    "- \"rare\": Occasional situations:\n",
    "  * \"winterizing cybertruck\"\n",
    "  * \"car wash settings?\"\n",
    "  * \"towing setup help\"\n",
    "\n",
    "- \"unlikely\": Technical/administrative:\n",
    "  * Manual details\n",
    "  * Specifications\n",
    "  * Legal info\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Provide your response in the following JSON format:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Natural, search-like question\",\n",
    "            \"answer\": \"Clear, helpful answer\",\n",
    "            \"supporting_text\": \"Relevant excerpt from source text\",\n",
    "            \"question_type\": \"factual\",\n",
    "            \"relevance_level\": \"common|rare|unlikely\",\n",
    "            \"relevance_reasoning\": \"Brief explanation of why this question fits the chosen relevance level\"\n",
    "        }}\n",
    "    ]\n",
    "}}\"\"\"\n",
    "\n",
    "REASONING_PROMPT = \"\"\"Generate 2-3 questions that real Cybertruck owners would ask when trying to understand how features work together or make decisions about using their vehicle. These should feel like real forum posts or search queries.\n",
    "\n",
    "Write questions as if they were being posted in an owners' forum. For example:\n",
    "\n",
    "Instead of:\n",
    "- \"What is the optimal charging strategy?\"\n",
    "- \"How does ambient temperature affect range?\"\n",
    "- \"What are the considerations for child safety?\"\n",
    "\n",
    "Write:\n",
    "- \"best way to charge for long road trip?\"\n",
    "- \"losing tons of range in cold - what helps?\"\n",
    "- \"safest seats for car seats?\"\n",
    "\n",
    "Make questions feel real by:\n",
    "1. Using natural patterns\n",
    "   - \"better to...\"\n",
    "   - \"best way to...\"\n",
    "   - \"tips for...\"\n",
    "2. Including context and emotion\n",
    "   - \"worried about range\"\n",
    "   - \"confused about charging\"\n",
    "   - \"need advice on settings\"\n",
    "3. Writing like real people\n",
    "   - Use contractions (I'm, won't, can't)\n",
    "   - OK to use incomplete sentences\n",
    "   - Include emotional context (\"Help!\", \"Confused about...\", \"Worried about...\")\n",
    "4. Adding situational details\n",
    "   - \"for camping\"\n",
    "   - \"in winter\"\n",
    "   - \"with full family\"\n",
    "\n",
    "For each question, evaluate its real-world relevance:\n",
    "- \"common\": Everyday decisions:\n",
    "  * \"faster charging vs battery life?\"\n",
    "  * \"seat heaters or cabin heat?\"\n",
    "  * \"best settings for commute\"\n",
    "\n",
    "- \"rare\": Occasional planning:\n",
    "  * \"road trip planning help\"\n",
    "  * \"winter driving tips\"\n",
    "  * \"towing affects on range?\"\n",
    "\n",
    "- \"unlikely\": Technical/theoretical:\n",
    "  * System details\n",
    "  * Technical specs\n",
    "  * Legal considerations\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Provide your response in the following JSON format:\n",
    "{{\n",
    "    \"questions\": [\n",
    "        {{\n",
    "            \"question\": \"Natural, forum-style question\",\n",
    "            \"answer\": \"Practical, helpful answer\",\n",
    "            \"supporting_text\": \"Relevant excerpt from source text\",\n",
    "            \"question_type\": \"reasoning\",\n",
    "            \"relevance_level\": \"common|rare|unlikely\",\n",
    "            \"relevance_reasoning\": \"Brief explanation of why this question fits the chosen relevance level\"\n",
    "        }}\n",
    "    ]\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a small number of questions to test the prompts\n",
    "# After inspecting the generated questions, we can adjust the prompts,\n",
    "# if necessary.\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a skilled question generator.\"},\n",
    "        {\"role\": \"user\", \"content\": FACTUAL_PROMPT.format(text=nodes[0].text)}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "factual_questions = json.loads(response.choices[0].message.content)\n",
    "print(\"Factual Questions Generated:\")\n",
    "print(json.dumps(factual_questions, indent=2))\n",
    "\n",
    "# Test reasoning questions\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a skilled question generator.\"},\n",
    "        {\"role\": \"user\", \"content\": REASONING_PROMPT.format(text=nodes[0].text)}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "reasoning_questions = json.loads(response.choices[0].message.content)\n",
    "print(\"\\nReasoning Questions Generated:\")\n",
    "print(json.dumps(reasoning_questions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random chunks of the document to generate evaluation\n",
    "# questions.\n",
    "\n",
    "import random\n",
    "NUM_CHUNKS = 10\n",
    "\n",
    "random_chunks = random.sample(nodes, NUM_CHUNKS)\n",
    "print(f\"Selected {len(random_chunks)} random chunks\")\n",
    "\n",
    "# 2. Generate Questions for Random Chunks\n",
    "candidate_examples = []\n",
    "\n",
    "for node in tqdm(random_chunks):\n",
    "    # Generate factual questions\n",
    "    factual_response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a skilled question generator.\"},\n",
    "            {\"role\": \"user\", \"content\": FACTUAL_PROMPT.format(text=node.text)}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    # Generate reasoning questions\n",
    "    reasoning_response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a skilled question generator.\"},\n",
    "            {\"role\": \"user\", \"content\": REASONING_PROMPT.format(text=node.text)}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    # Parse responses\n",
    "    factual_questions = json.loads(factual_response.choices[0].message.content)[\"questions\"]\n",
    "    reasoning_questions = json.loads(reasoning_response.choices[0].message.content)[\"questions\"]\n",
    "    \n",
    "    # Format and store\n",
    "    for question in factual_questions + reasoning_questions:\n",
    "        example = {\n",
    "            \"question\": question[\"question\"],\n",
    "            \"answer\": question[\"answer\"],\n",
    "            \"metadata\": {\n",
    "                \"chunk_id\": node.node_id,\n",
    "                \"question_type\": question[\"question_type\"],\n",
    "                \"supporting_text\": question[\"supporting_text\"],\n",
    "                \"relevance_level\": question[\"relevance_level\"],\n",
    "                \"source_position\": node.start_char_idx if hasattr(node, 'start_char_idx') else None,\n",
    "                \"filename\": node.metadata.get(\"filename\", \"unknown\")\n",
    "            }\n",
    "        }\n",
    "        candidate_examples.append(example)\n",
    "\n",
    "# 3. Filter for Common Questions\n",
    "common_examples = [ex for ex in candidate_examples \n",
    "                  if ex[\"metadata\"][\"relevance_level\"] == \"common\"]\n",
    "print(f\"\\nFound {len(common_examples)} common questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print selected examples for inspection\n",
    "\n",
    "import textwrap\n",
    "\n",
    "print(\"\\Generated common examples:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, example in enumerate(common_examples, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Answer: {example['answer']}\")\n",
    "    print(\"\\nMetadata:\")\n",
    "    print(f\"  Question Type: {example['metadata']['question_type']}\")\n",
    "    print(f\"  Relevance Level: {example['metadata']['relevance_level']}\")\n",
    "    print(f\"  Source File: {example['metadata']['filename']}\")\n",
    "    print(f\"  Chunk ID: {example['metadata']['chunk_id']}\")\n",
    "    print(\"\\nSupporting Text:\")\n",
    "    print(textwrap.fill(example['metadata']['supporting_text'], width=70))\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:21<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add the generated questions to the Langsmith dataset. Note:\n",
    "# first, create the dataset in Langsmith UI, and apply the chat schema.\n",
    "\n",
    "dataset_name = \"rag_evaluation_dataset\"\n",
    "\n",
    "for example in tqdm(common_examples):\n",
    "    # Format as chat input/output\n",
    "    input_data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"question\"]}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    output_data = {\n",
    "        \"message\": {\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": example[\"answer\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    langsmith_client.create_example(\n",
    "        dataset_name=dataset_name,\n",
    "        inputs=input_data,\n",
    "        outputs=output_data,\n",
    "        metadata=example[\"metadata\"]\n",
    "    )\n",
    "\n",
    "print(\"Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
